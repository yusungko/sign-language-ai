{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86f696a-8fde-4506-a472-c2607e560ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_dir = r\"C:\\Users\\User\\Desktop\\project\\videos\\test\"\n",
    "target_dir = r\"C:\\Users\\User\\Desktop\\project\\videos\"\n",
    "\n",
    "# Walk through all directories and subdirectories in the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp4\"):\n",
    "            # Construct the source file path\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Construct the target file path\n",
    "            target_file_path = os.path.join(target_dir, file)\n",
    "            \n",
    "            # Move the file to the target directory\n",
    "            shutil.move(source_file_path, target_file_path)\n",
    "source_dir = r\"C:\\Users\\User\\Desktop\\project\\videos\\val\"\n",
    "target_dir = r\"C:\\Users\\User\\Desktop\\project\\videos\"\n",
    "\n",
    "# Walk through all directories and subdirectories in the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp4\"):\n",
    "            # Construct the source file path\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Construct the target file path\n",
    "            target_file_path = os.path.join(target_dir, file)\n",
    "            \n",
    "            # Move the file to the target directory\n",
    "            shutil.move(source_file_path, target_file_path)\n",
    "source_dir = r\"C:\\Users\\User\\Desktop\\project\\videos\\train\"\n",
    "target_dir = r\"C:\\Users\\User\\Desktop\\project\\videos\"\n",
    "\n",
    "# Walk through all directories and subdirectories in the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp4\"):\n",
    "            # Construct the source file path\n",
    "            source_file_path = os.path.join(root, file)\n",
    "            \n",
    "            # Construct the target file path\n",
    "            target_file_path = os.path.join(target_dir, file)\n",
    "            \n",
    "            # Move the file to the target directory\n",
    "            shutil.move(source_file_path, target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d6c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import imageio\n",
    "from IPython import display\n",
    "from urllib import request\n",
    "from tensorflow_docs.vis import embed\n",
    "#tf.config.set_visible_devices([],'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f2b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(fname):\n",
    "  \"\"\" Retrieve the name of the class given a filename.\n",
    "\n",
    "    Args:\n",
    "      fname: Name of the file in the UCF101 dataset.\n",
    "\n",
    "    Returns:\n",
    "      Class that the file belongs to.\n",
    "  \"\"\"\n",
    "  return fname.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af6129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(fname):\n",
    "  \"\"\" Retrieve the name of the class given a filename.\n",
    "\n",
    "    Args:\n",
    "      fname: Name of the file in the UCF101 dataset.\n",
    "\n",
    "    Returns:\n",
    "      Class that the file belongs to.\n",
    "  \"\"\"\n",
    "  try:\n",
    "      return fname.split('_')[1]\n",
    "  except:\n",
    "      return \"nope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961f5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_per_class(files):\n",
    "    \"\"\" Retrieve the files that belong to each class.\n",
    "\n",
    "    Args:\n",
    "      files: List of files in the dataset.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of class names (key) and files (values). \n",
    "    \"\"\"\n",
    "    files_for_class = collections.defaultdict(list)\n",
    "    for fname in files:\n",
    "        if(get_position(fname)=='hands'):\n",
    "            class_name = get_class(fname)\n",
    "            files_for_class[class_name].append(fname)\n",
    "    return files_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753bae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 100\n",
    "FILES_PER_CLASS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2faf0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
    "  \"\"\" Create a dictionary with the class name and a subset of the files in that class.\n",
    "\n",
    "    Args:\n",
    "      files_for_class: Dictionary of class names (key) and files (values).\n",
    "      classes: List of classes.\n",
    "      files_per_class: Number of files per class of interest.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with class as key and list of specified number of video files in that class.\n",
    "  \"\"\"\n",
    "  files_subset = dict()\n",
    "\n",
    "  for class_name in classes:\n",
    "    class_files = files_for_class[class_name]\n",
    "    files_subset[class_name] = class_files[:files_per_class]\n",
    "\n",
    "  return files_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a674d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_zip(source_dir, to_dir, file_names):\n",
    "    \"\"\" Download the contents of the zip file from the zip URL.\n",
    "\n",
    "    Args:\n",
    "      zip_url: A URL with a zip file containing data.\n",
    "      to_dir: A directory to download data to.\n",
    "      file_names: Names of files to download.\n",
    "    \"\"\"\n",
    "    for fn in tqdm.tqdm(file_names):\n",
    "        class_name = get_class(fn)\n",
    "        source_file = source_dir / fn\n",
    "        output_file = to_dir / class_name / fn\n",
    "        source_file.rename(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41af1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_class_lists(files_for_class, count):\n",
    "  \"\"\" Returns the list of files belonging to a subset of data as well as the remainder of\n",
    "    files that need to be downloaded.\n",
    "    \n",
    "    Args:\n",
    "      files_for_class: Files belonging to a particular class of data.\n",
    "      count: Number of files to download.\n",
    "\n",
    "    Returns:\n",
    "      Files belonging to the subset of data and dictionary of the remainder of files that need to be downloaded.\n",
    "  \"\"\"\n",
    "  split_files = []\n",
    "  remainder = {}\n",
    "  for cls in files_for_class:\n",
    "    split_files.extend(files_for_class[cls][:count])\n",
    "    remainder[cls] = files_for_class[cls][count:]\n",
    "  return split_files, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9748f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ucf_100_subset(num_classes, splits, download_dir):\n",
    "  \"\"\" Download a subset of the UCF101 dataset and split them into various parts, such as\n",
    "    training, validation, and test.\n",
    "\n",
    "    Args:\n",
    "      zip_url: A URL with a ZIP file with the data.\n",
    "      num_classes: Number of labels.\n",
    "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
    "              (value is number of files per split).\n",
    "      download_dir: Directory to download data to.\n",
    "\n",
    "    Return:\n",
    "      Mapping of the directories containing the subsections of data.\n",
    "  \"\"\"\n",
    "  files = os.listdir(r\"C:\\Users\\User\\Desktop\\project\\videos\")\n",
    "  \n",
    "  files_for_class = get_files_per_class(files)\n",
    "\n",
    "  classes = list(files_for_class.keys())[:num_classes]\n",
    "\n",
    "  for cls in classes:\n",
    "    random.shuffle(files_for_class[cls])\n",
    "    \n",
    "  # Only use the number of classes you want in the dictionary\n",
    "  files_for_class = {x: files_for_class[x] for x in classes}\n",
    "\n",
    "  dirs = {}\n",
    "  for split_name, split_count in splits.items():\n",
    "    print(split_name, \":\")\n",
    "    split_dir = download_dir / split_name\n",
    "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
    "    download_from_zip(download_dir, split_dir, split_files)\n",
    "    dirs[split_name] = split_dir\n",
    "\n",
    "  return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c49f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2400/2400 [00:00<00:00, 2578.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 2836.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 2398.15it/s]\n"
     ]
    }
   ],
   "source": [
    "download_dir = pathlib.Path('C:/Users/User/Desktop/project/videos/')\n",
    "subset_paths = download_ucf_100_subset(num_classes = NUM_CLASSES,\n",
    "                                       splits = {\"train\": 24, \"val\": 8, \"test\": 8},\n",
    "                                       download_dir = download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "102a181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 4000\n"
     ]
    }
   ],
   "source": [
    "video_count_train = len(list(download_dir.glob('train/*/*.mp4')))\n",
    "video_count_val = len(list(download_dir.glob('val/*/*.mp4')))\n",
    "video_count_test = len(list(download_dir.glob('test/*/*.mp4')))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e6e60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "    \n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78160ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (112,112), frame_step = 1):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd71c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gif(images):\n",
    "  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n",
    "  imageio.mimsave('./animation.gif', converted_images, duration=20)\n",
    "  return embed.embed_file('./animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56bbfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames, training = False):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.path = path\n",
    "    self.n_frames = n_frames\n",
    "    self.training = training\n",
    "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "    video_paths = list(self.path.glob('*/*.mp4'))\n",
    "    classes = [p.parent.name for p in video_paths] \n",
    "    return video_paths, classes\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "    pairs = list(zip(video_paths, classes))\n",
    "\n",
    "    if self.training:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames) \n",
    "      label = self.class_ids_for_name[name] # Encode labels\n",
    "      yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dfcc6e0-4cab-481e-a84b-dc56f779c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 50, training = True),\n",
    "                                          output_signature = output_signature)\n",
    "\n",
    "# Batch the data\n",
    "train_ds = train_ds.batch(2)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 50),\n",
    "                                        output_signature = output_signature)\n",
    "val_ds = val_ds.batch(2)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], 50),\n",
    "                                         output_signature = output_signature)\n",
    "\n",
    "test_ds = test_ds.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1173e1b2-e0ba-4f8d-a7f5-56fe79ac59e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of one frame in the set of frames created\n",
    "HEIGHT = 112\n",
    "WIDTH = 112\n",
    "DEPTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eefd0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20ecd0a6-045e-4b88-ae47-d56a4bd82048",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, DEPTH, HEIGHT, WIDTH, 3)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "\n",
    "x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(input)\n",
    "x = layers.MaxPool3D(pool_size=2)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "# x = layers.MaxPool3D(pool_size=2)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPool3D(pool_size=2)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPool3D(pool_size=2)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(units=512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(units=100, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model.\n",
    "model = keras.Model(input, outputs, name=\"3dcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eb8f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_ds))\n",
    "model.build(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1543d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.000001), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f9f0437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50, 112, 112, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 48, 110, 110, 32)  2624      \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 24, 55, 55, 32)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 55, 55, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 22, 53, 53, 64)    55360     \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 11, 26, 26, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 11, 26, 26, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 9, 24, 24, 128)    221312    \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 4, 12, 12, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 4, 12, 12, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 73728)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               37749248  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               51300     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,080,740\n",
      "Trainable params: 38,080,292\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "550eeeef-1878-4f08-becd-92064809435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "keras.utils.plot_model(model, expand_nested=True, dpi=60, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebe6699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "    102/Unknown - 56s 475ms/step - loss: 15.8365 - accuracy: 0.0049"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 20, \n",
    "                    validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "  \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81611c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "  \n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944272d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "      \n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "      \n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b39521",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, 100, HEIGHT, WIDTH, 3)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(3)(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45aa99-ae99-4c9d-9ec8-8fb4e94e9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_ds))\n",
    "model.build(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8511a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3dcnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 25, 222, 222, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 23, 220, 220, 64)  5248      \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 11, 110, 110, 64)  0        \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 11, 110, 110, 64)  256      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 9, 108, 108, 128)  221312    \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 4, 54, 54, 128)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 4, 54, 54, 128)   512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 2, 52, 52, 256)    884992    \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 26, 26, 256)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1, 26, 26, 256)   1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 173056)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               88605184  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,719,041\n",
      "Trainable params: 89,718,145\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb6e22-1be6-4831-9b54-1e308481bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__MaxPool3DGrad_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1,64,23,220,220] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MaxPool3DGrad]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model, doing validation at the end of each epoch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ACER\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__MaxPool3DGrad_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1,64,23,220,220] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MaxPool3DGrad]"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 0.0001\n",
    "#lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    "#)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=True,\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"3d_image_classification.keras\", save_best_only=True\n",
    ")\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f36382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  \"\"\"\n",
    "    Plotting training and validation learning curves.\n",
    "\n",
    "    Args:\n",
    "      history: model history with all the metric measures\n",
    "  \"\"\"\n",
    "  fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "  fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "  # Plot loss\n",
    "  ax1.set_title('Loss')\n",
    "  ax1.plot(history.history['loss'], label = 'train')\n",
    "  ax1.plot(history.history['val_loss'], label = 'test')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  \n",
    "  # Determine upper bound of y-axis\n",
    "  max_loss = max(history.history['loss'] + history.history['val_loss'])\n",
    "\n",
    "  ax1.set_ylim([0, np.ceil(max_loss)])\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.legend(['Train', 'Validation']) \n",
    "\n",
    "  # Plot accuracy\n",
    "  ax2.set_title('Accuracy')\n",
    "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
    "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_ylim([0, 1])\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a8fef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('train_loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(history.history[\"loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
